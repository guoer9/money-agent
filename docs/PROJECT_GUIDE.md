# Qwen3-8B æ–°é—»åˆ†ç±»æœåŠ¡ - å®Œæ•´é…ç½®ä¸APIæŒ‡å—

[![Hugging Face](https://img.shields.io/badge/ğŸ¤—%20Model-qwen3--8b--news--classifier-yellow)](https://huggingface.co/guoer9/qwen3-8b-news-classifier)
[![GitHub](https://img.shields.io/badge/GitHub-vllm--branch-blue)](https://github.com/guoer9/money-agent/tree/vllm)

---

## ç›®å½•

1. [é¡¹ç›®æ¦‚è¿°](#1-é¡¹ç›®æ¦‚è¿°)
2. [ç¯å¢ƒé…ç½®](#2-ç¯å¢ƒé…ç½®)
3. [æœåŠ¡é…ç½®è¯¦è§£](#3-æœåŠ¡é…ç½®è¯¦è§£)
4. [APIæ¥å£è¯¦è§£](#4-apiæ¥å£è¯¦è§£)
5. [Metricsç›‘æ§](#5-metricsç›‘æ§)
6. [Kuberneteséƒ¨ç½²](#6-kuberneteséƒ¨ç½²)
7. [ä½¿ç”¨ç¤ºä¾‹](#7-ä½¿ç”¨ç¤ºä¾‹)
8. [æ•…éšœæ’æŸ¥](#8-æ•…éšœæ’æŸ¥)

---

## 1. é¡¹ç›®æ¦‚è¿°

### 1.1 é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®æ˜¯åŸºäº **Qwen3-8B** å¾®è°ƒçš„ä¸­æ–‡æ–°é—»åˆ†ç±»æ¨¡å‹æœåŠ¡ï¼Œåœ¨ TNEWS æ•°æ®é›†ä¸Šè¾¾åˆ° **62.4% å‡†ç¡®ç‡**ï¼Œè¶…è¶Š ERNIE 3.0 Titan (260B) ç­‰ SOTA æ¨¡å‹ã€‚

### 1.2 æ ¸å¿ƒç‰¹æ€§

| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| æ¨¡å‹ | Qwen3-8B + LoRA å¾®è°ƒ |
| é‡åŒ– | 8-bit (BitsAndBytes) |
| æ˜¾å­˜ | ~10GB (æ”¯æŒ RTX 3080/4090) |
| å¹¶å‘ | 3 ä¸ªå¹¶å‘è¯·æ±‚ |
| API | OpenAI å…¼å®¹æ ¼å¼ |
| ç›‘æ§ | Prometheus é›†æˆ |
| éƒ¨ç½² | Docker + Kubernetes |

### 1.3 é¡¹ç›®ç»“æ„

```
qwen_vllm/
â”œâ”€â”€ config.py                    # æœåŠ¡é…ç½®æ–‡ä»¶
â”œâ”€â”€ Dockerfile                   # å®¹å™¨æ„å»ºæ–‡ä»¶
â”œâ”€â”€ docker-compose.yml           # Docker Compose é…ç½®
â”œâ”€â”€ start.sh                     # æœ¬åœ°å¯åŠ¨è„šæœ¬
â”œâ”€â”€ deploy-k8s.sh               # K8s éƒ¨ç½²è„šæœ¬
â”œâ”€â”€ requirements.txt            # Python ä¾èµ–
â”‚
â”œâ”€â”€ models/                     # æ¨¡å‹æ–‡ä»¶
â”‚   â””â”€â”€ qwen-news-classifier-merged/  # å¾®è°ƒåçš„æ¨¡å‹ (16GB)
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ deployment/             # éƒ¨ç½²ç›¸å…³
â”‚   â”‚   â”œâ”€â”€ deploy_with_limits.py    # ä¸»æœåŠ¡ (Flask)
â”‚   â”‚   â”œâ”€â”€ metrics.py              # Metrics æ”¶é›†
â”‚   â”‚   â”œâ”€â”€ monitor_metrics.py      # å®æ—¶ç›‘æ§
â”‚   â”‚   â””â”€â”€ test_api.py             # API æµ‹è¯•
â”‚   â”œâ”€â”€ training/               # è®­ç»ƒè„šæœ¬
â”‚   â”‚   â”œâ”€â”€ train_qwen.py          # å¾®è°ƒè®­ç»ƒ
â”‚   â”‚   â”œâ”€â”€ prepare_data.py        # æ•°æ®å‡†å¤‡
â”‚   â”‚   â””â”€â”€ inference_qwen.py      # æ¨ç†æµ‹è¯•
â”‚   â””â”€â”€ utils/                  # å·¥å…·è„šæœ¬
â”‚
â”œâ”€â”€ k8s/                        # Kubernetes é…ç½®
â”‚   â”œâ”€â”€ base/                   # åŸºç¡€é…ç½®
â”‚   â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”‚   â”œâ”€â”€ service.yaml
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ overlays/production/    # ç”Ÿäº§ç¯å¢ƒè¦†ç›–
â”‚
â””â”€â”€ docs/                       # æ–‡æ¡£
```

---

## 2. ç¯å¢ƒé…ç½®

### 2.1 ç¡¬ä»¶è¦æ±‚

| ç»„ä»¶ | æœ€ä½é…ç½® | æ¨èé…ç½® |
|------|----------|----------|
| GPU | RTX 3080 10GB | RTX 4090 24GB |
| å†…å­˜ | 16GB | 32GB |
| ç£ç›˜ | 30GB | 50GB |
| CPU | 8æ ¸ | 16æ ¸ |

### 2.2 è½¯ä»¶ä¾èµ–

```bash
# Python ç‰ˆæœ¬
Python >= 3.9

# CUDA ç‰ˆæœ¬
CUDA >= 12.0

# ä¸»è¦ä¾èµ–
torch >= 2.0
transformers >= 4.40
bitsandbytes >= 0.43
flask >= 2.0
flask-limiter >= 3.0
gunicorn >= 21.0
```

### 2.3 å®‰è£…æ­¥éª¤

```bash
# 1. å…‹éš†é¡¹ç›®
git clone -b vllm https://github.com/guoer9/money-agent.git
cd money-agent

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n vllm-deploy python=3.10
conda activate vllm-deploy

# 3. å®‰è£…ä¾èµ–
pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/

# 4. ä¸‹è½½æ¨¡å‹ (ä» Hugging Face)
pip install huggingface_hub
python -c "
from huggingface_hub import snapshot_download
snapshot_download('guoer9/qwen3-8b-news-classifier', local_dir='models/qwen-news-classifier-merged')
"

# 5. å¯åŠ¨æœåŠ¡
bash start.sh
```

---

## 3. æœåŠ¡é…ç½®è¯¦è§£

### 3.1 é…ç½®æ–‡ä»¶ (`config.py`)

```python
# ============================================
# æœåŠ¡é…ç½®
# ============================================

# æœåŠ¡åœ°å€
HOST = "0.0.0.0"          # ç›‘å¬æ‰€æœ‰ç½‘å¡
PORT = 8000               # æœåŠ¡ç«¯å£

# æ¨¡å‹è·¯å¾„
MODEL_PATH = "./models/qwen-news-classifier-merged"

# ============================================
# å¹¶å‘æ§åˆ¶ï¼ˆåŸºäºæ˜¾å­˜ä¼˜åŒ–ï¼‰
# ============================================

MAX_CONCURRENT_REQUESTS = 3   # æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
                              # RTX 3080 10GB: å»ºè®® 2-3
                              # RTX 4090 24GB: å»ºè®® 4-6

REQUEST_QUEUE_SIZE = 10       # è¯·æ±‚é˜Ÿåˆ—å¤§å°
                              # è¶…è¿‡åè¿”å› 503

# ============================================
# é€Ÿç‡é™åˆ¶
# ============================================

RATE_LIMIT_PER_MINUTE = 10    # æ¯IPæ¯åˆ†é’Ÿè¯·æ±‚æ•°
RATE_LIMIT_PER_HOUR = 100     # æ¯IPæ¯å°æ—¶è¯·æ±‚æ•°

# ============================================
# æ¨ç†å‚æ•°
# ============================================

MAX_TOKENS = 512              # æœ€å¤§ç”Ÿæˆtokenæ•°
DEFAULT_TEMPERATURE = 0.3     # é»˜è®¤æ¸©åº¦ (è¶Šä½è¶Šç¡®å®š)
TIMEOUT_SECONDS = 120         # è¯·æ±‚è¶…æ—¶æ—¶é—´

# ============================================
# Gunicorn é…ç½®
# ============================================

WORKERS = 1                   # Workeræ•°é‡ (GPUæ¨¡å‹å»ºè®®1)
THREADS = 4                   # æ¯ä¸ªworkerçš„çº¿ç¨‹æ•°
```

### 3.2 é…ç½®è¯´æ˜

#### å¹¶å‘æ§åˆ¶åŸç†

```
è¯·æ±‚ â†’ [é˜Ÿåˆ—] â†’ [ä¿¡å·é‡æ§åˆ¶] â†’ [GPUæ¨ç†] â†’ å“åº”
         â†“           â†“
      æœ€å¤§10ä¸ª    æœ€å¤§3ä¸ªå¹¶å‘
```

- **REQUEST_QUEUE_SIZE**: ç­‰å¾…é˜Ÿåˆ—å¤§å°ï¼Œè¶…è¿‡è¿”å› 503
- **MAX_CONCURRENT_REQUESTS**: åŒæ—¶è¿›è¡Œ GPU æ¨ç†çš„è¯·æ±‚æ•°
- **ä¿¡å·é‡æœºåˆ¶**: ä½¿ç”¨ `threading.Semaphore` æ§åˆ¶ GPU è®¿é—®

#### æ˜¾å­˜ä¸å¹¶å‘çš„å…³ç³»

| GPU | æ˜¾å­˜ | å»ºè®®å¹¶å‘ | è¯´æ˜ |
|-----|------|----------|------|
| RTX 3080 | 10GB | 2-3 | 8-bité‡åŒ–åçº¦7GB |
| RTX 3090 | 24GB | 4-6 | ä½™é‡å……è¶³ |
| RTX 4090 | 24GB | 5-8 | æ¨ç†æ›´å¿« |
| RTX 5090 | 32GB | 8-10 | é«˜æ€§èƒ½ |

### 3.3 é‡åŒ–é…ç½®

```python
# 8-bit é‡åŒ–é…ç½® (deploy_with_limits.py)
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,                    # å¯ç”¨8-bité‡åŒ–
    llm_int8_threshold=6.0,               # å¼‚å¸¸å€¼é˜ˆå€¼
    llm_int8_has_fp16_weight=False,       # æƒé‡æ ¼å¼
    llm_int8_enable_fp32_cpu_offload=True # CPU offload
)
```

**é‡åŒ–æ•ˆæœ**:
- åŸå§‹æ¨¡å‹: ~16GB
- 8-bité‡åŒ–å: ~8GB
- æ¨ç†æ˜¾å­˜: ~10GB (å«KV cache)

---

## 4. APIæ¥å£è¯¦è§£

### 4.1 æ¥å£æ€»è§ˆ

| ç«¯ç‚¹ | æ–¹æ³• | è¯´æ˜ |
|------|------|------|
| `/v1/chat/completions` | POST | å¯¹è¯è¡¥å…¨ (æ¨è) |
| `/v1/completions` | POST | æ–‡æœ¬è¡¥å…¨ |
| `/v1/models` | GET | æ¨¡å‹åˆ—è¡¨ |
| `/health` | GET | å¥åº·æ£€æŸ¥ |
| `/stats` | GET | ç»Ÿè®¡ä¿¡æ¯ |
| `/metrics` | GET | Metrics (JSON) |
| `/metrics/prometheus` | GET | Metrics (Prometheus) |

### 4.2 å¯¹è¯è¡¥å…¨æ¥å£ (æ¨è)

**ç«¯ç‚¹**: `POST /v1/chat/completions`

**è¯·æ±‚æ ¼å¼**:
```json
{
    "messages": [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ–°é—»åˆ†ç±»åŠ©æ‰‹"},
        {"role": "user", "content": "è¯·åˆ†ç±»è¿™æ¡æ–°é—»ï¼šå¤®è¡Œå®£å¸ƒé™æ¯25ä¸ªåŸºç‚¹"}
    ],
    "max_tokens": 100,
    "temperature": 0.3
}
```

**å‚æ•°è¯´æ˜**:

| å‚æ•° | ç±»å‹ | å¿…å¡« | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|------|--------|------|
| messages | array | âœ… | - | å¯¹è¯æ¶ˆæ¯åˆ—è¡¨ |
| max_tokens | int | âŒ | 100 | æœ€å¤§ç”Ÿæˆtokenæ•° (ä¸Šé™512) |
| temperature | float | âŒ | 0.7 | æ¸©åº¦ (0-1, è¶Šä½è¶Šç¡®å®š) |

**messages æ ¼å¼**:
```json
[
    {"role": "system", "content": "ç³»ç»Ÿæç¤º"},
    {"role": "user", "content": "ç”¨æˆ·è¾“å…¥"},
    {"role": "assistant", "content": "åŠ©æ‰‹å›å¤"},
    {"role": "user", "content": "åç»­é—®é¢˜"}
]
```

**å“åº”æ ¼å¼**:
```json
{
    "id": "chatcmpl-123456789",
    "object": "chat.completion",
    "created": 1703404800,
    "model": "qwen-news-classifier",
    "choices": [
        {
            "index": 0,
            "message": {
                "role": "assistant",
                "content": "è¿™æ¡æ–°é—»å±äºã€è´¢ç»ã€‘ç±»åˆ«ã€‚\n\nåŸå› ï¼šæ–°é—»å†…å®¹æ¶‰åŠå¤®è¡Œè´§å¸æ”¿ç­–..."
            },
            "finish_reason": "stop"
        }
    ],
    "usage": {
        "prompt_tokens": 50,
        "completion_tokens": 80,
        "total_tokens": 130
    }
}
```

### 4.3 æ–‡æœ¬è¡¥å…¨æ¥å£

**ç«¯ç‚¹**: `POST /v1/completions`

**è¯·æ±‚æ ¼å¼**:
```json
{
    "prompt": "è¯·åˆ†ç±»ä»¥ä¸‹æ–°é—»ï¼šè‹¹æœå…¬å¸å‘å¸ƒæ–°æ¬¾iPhoneï¼Œè‚¡ä»·ä¸Šæ¶¨5%ã€‚\nç±»åˆ«ï¼š",
    "max_tokens": 50,
    "temperature": 0.3
}
```

**å“åº”æ ¼å¼**:
```json
{
    "id": "cmpl-123456789",
    "object": "text_completion",
    "created": 1703404800,
    "model": "qwen-news-classifier",
    "choices": [
        {
            "text": "ç§‘æŠ€/è´¢ç»",
            "index": 0,
            "finish_reason": "stop"
        }
    ],
    "usage": {
        "prompt_tokens": 30,
        "completion_tokens": 5,
        "total_tokens": 35
    }
}
```

### 4.4 å¥åº·æ£€æŸ¥æ¥å£

**ç«¯ç‚¹**: `GET /health`

**å“åº”**:
```json
{
    "status": "ok",
    "gpu": {
        "allocated_gb": 7.52,
        "reserved_gb": 8.0,
        "free_gb": 2.48
    },
    "limits": {
        "max_concurrent_requests": 3,
        "max_queue_size": 10,
        "rate_limit": "10 requests/minute per IP"
    }
}
```

### 4.5 ç»Ÿè®¡ä¿¡æ¯æ¥å£

**ç«¯ç‚¹**: `GET /stats`

**å“åº”**:
```json
{
    "statistics": {
        "total_requests": 1250,
        "successful_requests": 1200,
        "failed_requests": 50,
        "queue_full_count": 15,
        "current_queue_size": 2,
        "peak_queue_size": 8
    },
    "current_queue_size": 2,
    "available_slots": 1
}
```

### 4.6 æ¨¡å‹åˆ—è¡¨æ¥å£

**ç«¯ç‚¹**: `GET /v1/models`

**å“åº”**:
```json
{
    "object": "list",
    "data": [
        {
            "id": "qwen-news-classifier",
            "object": "model",
            "created": 0,
            "owned_by": "user"
        }
    ]
}
```

### 4.7 é”™è¯¯å“åº”

**é€Ÿç‡é™åˆ¶ (429)**:
```json
{
    "error": "Rate limit exceeded"
}
```

**é˜Ÿåˆ—å·²æ»¡ (503)**:
```json
{
    "error": "æœåŠ¡ç¹å¿™ï¼Œè¯·ç¨åé‡è¯•",
    "queue_size": 10,
    "max_queue_size": 10
}
```

**å†…éƒ¨é”™è¯¯ (500)**:
```json
{
    "error": "CUDA out of memory..."
}
```

---

## 5. Metricsç›‘æ§

### 5.1 æ ¸å¿ƒæŒ‡æ ‡

| æŒ‡æ ‡ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `num_waiting_requests` | Gauge | ç­‰å¾…é˜Ÿåˆ—ä¸­çš„è¯·æ±‚æ•° |
| `num_running_requests` | Gauge | æ­£åœ¨å¤„ç†çš„è¯·æ±‚æ•° |
| `total_requests` | Counter | æ€»è¯·æ±‚æ•° |
| `ttft_mean` | Gauge | å¹³å‡é¦–Tokenå»¶è¿Ÿ (ç§’) |
| `ttft_p50/p95/p99` | Gauge | TTFT ç™¾åˆ†ä½æ•° |
| `decoding_throughput_mean` | Gauge | å¹³å‡è§£ç ååé‡ (tokens/ç§’) |
| `total_throughput` | Gauge | æ€»ååé‡ (tokens/ç§’) |

### 5.2 Metrics æ¥å£

#### JSON æ ¼å¼

**ç«¯ç‚¹**: `GET /metrics`

```json
{
    "num_waiting_requests": 0,
    "num_running_requests": 1,
    "total_requests": 150,
    "total_tokens_generated": 12500,
    "slo": {
        "ttft_mean": 0.45,
        "ttft_p50": 0.42,
        "ttft_p95": 0.68,
        "ttft_p99": 0.85,
        "decoding_throughput_mean": 48.5,
        "decoding_throughput_p50": 50.2,
        "decoding_throughput_p95": 42.1,
        "total_throughput": 45.3,
        "sample_size": 100
    }
}
```

#### Prometheus æ ¼å¼

**ç«¯ç‚¹**: `GET /metrics/prometheus`

```
# HELP vllm_num_waiting_requests Number of requests waiting in queue
# TYPE vllm_num_waiting_requests gauge
vllm_num_waiting_requests 0

# HELP vllm_num_running_requests Number of requests currently running
# TYPE vllm_num_running_requests gauge
vllm_num_running_requests 1

# HELP vllm_ttft_p95 P95 time to first token in seconds
# TYPE vllm_ttft_p95 gauge
vllm_ttft_p95 0.68

# HELP vllm_decoding_throughput_mean Mean decoding throughput in tokens/sec
# TYPE vllm_decoding_throughput_mean gauge
vllm_decoding_throughput_mean 48.5
```

### 5.3 å®æ—¶ç›‘æ§

```bash
# ä½¿ç”¨ç›‘æ§è„šæœ¬
python scripts/deployment/monitor_metrics.py

# è¾“å‡ºç¤ºä¾‹:
# ========================================
# vLLM Metrics Monitor (æ¯5ç§’åˆ·æ–°)
# ========================================
# 
# ğŸ“Š å½“å‰çŠ¶æ€:
#   ç­‰å¾…è¯·æ±‚: 0
#   è¿è¡Œè¯·æ±‚: 1
#   æ€»è¯·æ±‚æ•°: 150
# 
# â±ï¸ SLOæŒ‡æ ‡:
#   TTFT P50: 0.42s
#   TTFT P95: 0.68s
#   ååé‡: 48.5 tokens/s
```

### 5.4 Prometheus é…ç½®

```yaml
# prometheus.yml
scrape_configs:
  - job_name: 'vllm'
    static_configs:
      - targets: ['localhost:8000']
    metrics_path: '/metrics/prometheus'
    scrape_interval: 15s
```

### 5.5 å‘Šè­¦è§„åˆ™

```yaml
# æ¨èå‘Šè­¦è§„åˆ™
groups:
  - name: vllm
    rules:
      - alert: HighQueueLength
        expr: vllm_num_waiting_requests > 5
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "è¯·æ±‚é˜Ÿåˆ—è¿‡é•¿"
          
      - alert: HighTTFT
        expr: vllm_ttft_p95 > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "TTFTå»¶è¿Ÿè¿‡é«˜"
```

---

## 6. Kuberneteséƒ¨ç½²

### 6.1 å¿«é€Ÿéƒ¨ç½²

```bash
# 1. æ„å»ºé•œåƒ
docker build -t qwen-vllm:latest .

# 2. éƒ¨ç½²åˆ°K8s
kubectl apply -k k8s/base

# 3. æŸ¥çœ‹çŠ¶æ€
kubectl get pods -l app=qwen-vllm

# 4. è®¿é—®æœåŠ¡
kubectl port-forward svc/qwen-vllm 8000:8000
```

### 6.2 Deployment é…ç½®

```yaml
# k8s/base/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: qwen-vllm
spec:
  replicas: 1
  template:
    spec:
      containers:
      - name: qwen-vllm
        image: docker.io/library/qwen-vllm:latest
        imagePullPolicy: Never
        ports:
        - containerPort: 8000
        env:
        - name: VLLM_MODEL
          value: "/app/models/qwen-news-classifier-merged"
        resources:
          requests:
            memory: "16Gi"
            cpu: "4"
            nvidia.com/gpu: "1"
          limits:
            memory: "32Gi"
            cpu: "8"
            nvidia.com/gpu: "1"
        volumeMounts:
        - name: model-storage
          mountPath: /app/models
      volumes:
      - name: model-storage
        hostPath:
          path: /home/zch/qwen_vllm/models/qwen-news-classifier-merged
```

### 6.3 Service é…ç½®

```yaml
# k8s/base/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: qwen-vllm
spec:
  type: ClusterIP
  ports:
  - port: 8000
    targetPort: 8000
  selector:
    app: qwen-vllm
```

---

## 7. ä½¿ç”¨ç¤ºä¾‹

### 7.1 Python è°ƒç”¨

```python
import requests

url = "http://localhost:8000/v1/chat/completions"

# æ–°é—»åˆ†ç±»
response = requests.post(url, json={
    "messages": [
        {"role": "user", "content": "è¯·åˆ†ç±»è¿™æ¡æ–°é—»ï¼šå¤®è¡Œå®£å¸ƒé™æ¯25ä¸ªåŸºç‚¹ï¼Œå¸‚åœºååº”ç§¯æ"}
    ],
    "max_tokens": 100,
    "temperature": 0.3
})

result = response.json()
print(result['choices'][0]['message']['content'])
# è¾“å‡º: è¿™æ¡æ–°é—»å±äºã€è´¢ç»ã€‘ç±»åˆ«...
```

### 7.2 cURL è°ƒç”¨

```bash
# æ–°é—»åˆ†ç±»
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "è¯·åˆ†ç±»ï¼šç‰¹æ–¯æ‹‰è‚¡ä»·å¤§æ¶¨10%"}
    ],
    "max_tokens": 100
  }'

# å¥åº·æ£€æŸ¥
curl http://localhost:8000/health

# æŸ¥çœ‹ç»Ÿè®¡
curl http://localhost:8000/stats

# PrometheusæŒ‡æ ‡
curl http://localhost:8000/metrics/prometheus
```

### 7.3 æ‰¹é‡å¤„ç†

```python
import requests
from concurrent.futures import ThreadPoolExecutor

def classify_news(news):
    response = requests.post(
        "http://localhost:8000/v1/chat/completions",
        json={
            "messages": [{"role": "user", "content": f"åˆ†ç±»ï¼š{news}"}],
            "max_tokens": 50,
            "temperature": 0.3
        },
        timeout=30
    )
    return response.json()['choices'][0]['message']['content']

news_list = [
    "å¤®è¡Œé™æ¯25ä¸ªåŸºç‚¹",
    "è‹¹æœå‘å¸ƒæ–°æ¬¾iPhone",
    "å›½è¶³äºšæ´²æ¯å‡ºå±€",
    # ...
]

# å¹¶å‘å¤„ç† (å»ºè®®ä¸è¶…è¿‡3ä¸ªå¹¶å‘)
with ThreadPoolExecutor(max_workers=3) as executor:
    results = list(executor.map(classify_news, news_list))
```

### 7.4 æµå¼å“åº” (å¼€å‘ä¸­)

```python
# æ³¨æ„: å½“å‰ç‰ˆæœ¬æš‚ä¸æ”¯æŒæµå¼å“åº”
# å¦‚éœ€æµå¼å“åº”ï¼Œå»ºè®®ä½¿ç”¨ vLLM å®˜æ–¹å¼•æ“
```

---

## 8. æ•…éšœæ’æŸ¥

### 8.1 å¸¸è§é—®é¢˜

| é—®é¢˜ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| CUDA out of memory | æ˜¾å­˜ä¸è¶³ | é™ä½ `MAX_CONCURRENT_REQUESTS` |
| 503 æœåŠ¡ç¹å¿™ | é˜Ÿåˆ—å·²æ»¡ | ç­‰å¾…æˆ–å¢åŠ é˜Ÿåˆ—å¤§å° |
| 429 é€Ÿç‡é™åˆ¶ | è¯·æ±‚è¿‡å¿« | é™ä½è¯·æ±‚é¢‘ç‡ |
| è¿æ¥è¶…æ—¶ | æ¨ç†æ—¶é—´é•¿ | å¢åŠ è¶…æ—¶æ—¶é—´ |
| æ¨¡å‹åŠ è½½å¤±è´¥ | è·¯å¾„é”™è¯¯ | æ£€æŸ¥ `MODEL_PATH` |

### 8.2 æ—¥å¿—æŸ¥çœ‹

```bash
# æœ¬åœ°æœåŠ¡
tail -f logs/vllm.log

# K8s Pod
kubectl logs -f deployment/qwen-vllm

# å®æ—¶GPUçŠ¶æ€
watch -n 1 nvidia-smi
```

### 8.3 æ€§èƒ½è°ƒä¼˜

```python
# config.py è°ƒä¼˜å»ºè®®

# é«˜åååœºæ™¯
MAX_CONCURRENT_REQUESTS = 5
REQUEST_QUEUE_SIZE = 20
MAX_TOKENS = 256  # å‡å°‘æœ€å¤§token

# ä½å»¶è¿Ÿåœºæ™¯
MAX_CONCURRENT_REQUESTS = 2
REQUEST_QUEUE_SIZE = 5
DEFAULT_TEMPERATURE = 0.1  # æ›´ç¡®å®šçš„è¾“å‡º
```

### 8.4 å¥åº·æ£€æŸ¥

```bash
# å®Œæ•´å¥åº·æ£€æŸ¥è„šæœ¬
#!/bin/bash
echo "=== å¥åº·æ£€æŸ¥ ==="

# 1. æœåŠ¡çŠ¶æ€
curl -s http://localhost:8000/health | jq

# 2. GPUçŠ¶æ€
nvidia-smi --query-gpu=memory.used,memory.free --format=csv

# 3. è¿›ç¨‹çŠ¶æ€
ps aux | grep gunicorn

# 4. é˜Ÿåˆ—çŠ¶æ€
curl -s http://localhost:8000/stats | jq
```

---

## é™„å½•

### A. ç¯å¢ƒå˜é‡

| å˜é‡ | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `VLLM_MODEL` | æ¨¡å‹è·¯å¾„ | `./models/qwen-news-classifier-merged` |
| `VLLM_PORT` | æœåŠ¡ç«¯å£ | `8000` |
| `CUDA_VISIBLE_DEVICES` | GPUè®¾å¤‡ | `0` |

### B. ç›¸å…³é“¾æ¥

- **æ¨¡å‹**: https://huggingface.co/guoer9/qwen3-8b-news-classifier
- **ä»£ç **: https://github.com/guoer9/money-agent/tree/vllm
- **åŸºç¡€æ¨¡å‹**: https://huggingface.co/Qwen/Qwen3-8B

### C. ç‰ˆæœ¬å†å²

| ç‰ˆæœ¬ | æ—¥æœŸ | è¯´æ˜ |
|------|------|------|
| v1.0 | 2024-12-24 | åˆå§‹ç‰ˆæœ¬ï¼ŒK8séƒ¨ç½² |

---

*æ–‡æ¡£æ›´æ–°: 2024-12-24*
